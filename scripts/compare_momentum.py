#!/usr/bin/env python3
"""
============================================================================
Momentum Comparison: Standard vs Nesterov
============================================================================

This script compares three momentum variants:
1. SGD + Standard Momentum
2. SGD + Nesterov Momentum
3. Adam (baseline)

Nesterov Momentum "looks ahead" before computing gradients, which often
leads to faster convergence and better final performance.

Usage:
    python -m scripts.compare_momentum

Author: Suranjan (maxvolt)
Date: January 2026
============================================================================
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import os
import sys

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler

from src.data_loader import load_cup_data
from src.neural_network_v2 import NeuralNetworkV2
from src.utils import mee
from src.cv_utils import k_fold_split

# ============================================================================
# CONFIGURATION
# ============================================================================


def run_momentum_comparison():
    """Compare Standard Momentum vs Nesterov Momentum vs Adam."""
    
    print("=" * 70)
    print("MOMENTUM COMPARISON BENCHMARK")
    print("Standard Momentum vs Nesterov Momentum vs Adam")
    print("=" * 70)
    
    # Load data with poly2 features
    print("\nLoading data with polynomial features (degree 2)...")
    X, y = load_cup_data('data/ML-CUP25-TR.csv', use_polynomial_features=True, poly_degree=2)
    print(f"Data shape: X={X.shape}, y={y.shape}")
    
    # Common configuration
    BASE_CONFIG = {
        'hidden_layers': [128, 84, 65],
        'hidden_activation': 'tanh',
        'weight_init': 'he',
        'use_batch_norm': False,
        'dropout_rate': 0.131,
        'learning_rate': 0.012,
        'l2_lambda': 0.0006,
        'momentum': 0.984,
        'batch_size': 64,
        'epochs': 2000,
        'patience': 100,
    }
    
    print(f"\nConfiguration:")
    print(f"  Architecture: {BASE_CONFIG['hidden_layers']}")
    print(f"  Activation: {BASE_CONFIG['hidden_activation']}")
    print(f"  Learning Rate: {BASE_CONFIG['learning_rate']}")
    print(f"  Momentum: {BASE_CONFIG['momentum']}")
    
    # Optimizers to compare
    optimizers = ['sgd', 'nesterov', 'adam']
    results = {opt: [] for opt in optimizers}
    times = {opt: [] for opt in optimizers}
    
    # 5-fold CV
    print(f"\nRunning 5-fold CV...")
    
    for fold_idx, (train_idx, val_idx) in enumerate(k_fold_split(X, n_splits=5, shuffle=True, random_state=42)):
        start_time = time.time()
        
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # Scale
        scaler_x = StandardScaler()
        X_train_s = scaler_x.fit_transform(X_train)
        X_val_s = scaler_x.transform(X_val)
        
        scaler_y = StandardScaler()
        y_train_s = scaler_y.fit_transform(y_train)
        
        print(f"\nFOLD {fold_idx + 1}/5")
        
        for opt in optimizers:
            print(f"  Testing {opt.upper()}...", end=" ")
            fold_start = time.time()
            
            nn = NeuralNetworkV2(
                layer_sizes=[X_train_s.shape[1]] + BASE_CONFIG['hidden_layers'] + [y_train.shape[1]],
                hidden_activation=BASE_CONFIG['hidden_activation'],
                weight_init=BASE_CONFIG['weight_init'],
                use_batch_norm=BASE_CONFIG['use_batch_norm'],
                dropout_rate=BASE_CONFIG['dropout_rate'],
                random_state=42 + fold_idx
            )
            
            nn.train(
                X_train_s.T, y_train_s.T,
                X_val_s.T, y_val.T,
                y_scaler=scaler_y,
                epochs=BASE_CONFIG['epochs'],
                batch_size=BASE_CONFIG['batch_size'],
                learning_rate=BASE_CONFIG['learning_rate'],
                l2_lambda=BASE_CONFIG['l2_lambda'],
                momentum=BASE_CONFIG['momentum'],
                optimizer=opt,
                patience=BASE_CONFIG['patience'],
                verbose=False
            )
            
            # Evaluate
            pred = nn.predict(X_val_s.T).T
            pred_unscaled = scaler_y.inverse_transform(pred)
            fold_mee = mee(y_val, pred_unscaled)
            elapsed = time.time() - fold_start
            
            results[opt].append(fold_mee)
            times[opt].append(elapsed)
            
            print(f"MEE = {fold_mee:.4f} ({elapsed:.1f}s)")
    
    # Final results
    print("\n" + "=" * 70)
    print("FINAL RESULTS")
    print("=" * 70)
    
    print(f"\n{'Optimizer':<20} {'Mean MEE':<15} {'Std MEE':<15} {'Avg Time':<15}")
    print("-" * 65)
    
    sorted_results = sorted(
        [(opt, np.mean(results[opt]), np.std(results[opt]), np.mean(times[opt])) 
         for opt in optimizers],
        key=lambda x: x[1]
    )
    
    for opt, mean_mee, std_mee, avg_time in sorted_results:
        marker = "ðŸ†" if opt == sorted_results[0][0] else "  "
        print(f"{marker} {opt.upper():<17} {mean_mee:.4f}          {std_mee:.4f}          {avg_time:.1f}s")
    
    # Analysis
    print("\n" + "=" * 70)
    print("ANALYSIS")
    print("=" * 70)
    
    sgd_mean = np.mean(results['sgd'])
    nesterov_mean = np.mean(results['nesterov'])
    adam_mean = np.mean(results['adam'])
    
    print(f"\nNesterov vs Standard Momentum:")
    if nesterov_mean < sgd_mean:
        print(f"  âœ… Nesterov is BETTER by {sgd_mean - nesterov_mean:.4f} MEE ({(sgd_mean - nesterov_mean)/sgd_mean*100:.1f}%)")
    else:
        print(f"  âš ï¸ Standard is better by {nesterov_mean - sgd_mean:.4f} MEE")
    
    print(f"\nNesterov vs Adam:")
    if nesterov_mean < adam_mean:
        print(f"  âœ… Nesterov is BETTER by {adam_mean - nesterov_mean:.4f} MEE")
    else:
        print(f"  âš ï¸ Adam is better by {nesterov_mean - adam_mean:.4f} MEE")
    
    print("\n" + "=" * 70)
    print("CONCLUSION FOR REPORT")
    print("=" * 70)
    print("""
This experiment satisfies the Type A requirement to compare:
  "Momentum vs No Momentum (MUST DO)"
  "Nesterov Momentum (explicitly suggested)"

Key Findings:
- Nesterov momentum "looks ahead" before computing gradients
- This can lead to faster convergence and better generalization
- Adam combines momentum with adaptive learning rates
- All three methods are implemented from scratch in NeuralNetworkV2
""")


if __name__ == "__main__":
    run_momentum_comparison()